{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.55.dev0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.vision.interpret import *\n",
    "import fastai; fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../../data/siim_acr_pneu/train'),\n",
       " PosixPath('../../data/siim_acr_pneu/clas_df.csv'),\n",
       " PosixPath('../../data/siim_acr_pneu/dicom-images-test'),\n",
       " PosixPath('../../data/siim_acr_pneu/models'),\n",
       " PosixPath('../../data/siim_acr_pneu/dicom-images-train'),\n",
       " PosixPath('../../data/siim_acr_pneu/train-rle.csv'),\n",
       " PosixPath('../../data/siim_acr_pneu/seg_df.csv'),\n",
       " PosixPath('../../data/siim_acr_pneu/learn'),\n",
       " PosixPath('../../data/siim_acr_pneu/test')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"../../data/siim_acr_pneu/\"); data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "- 1) Run segmentation on all valid and test images\n",
    "- 2) Run classification on all test valid and images\n",
    "- 3) Find optimal thresh\n",
    "- 4) Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is needed or div=False\n",
    "class SegmentationLabelList(SegmentationLabelList):\n",
    "    def open(self, fn): return open_mask(fn, div=True)\n",
    "\n",
    "class SegmentationItemList(SegmentationItemList):\n",
    "    _label_cls = SegmentationLabelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_learn = Learner(**torch.load(data_path/f'learn/clas-chexpert-ft-resnet34-{sz}'))\n",
    "seg_learn = Learner(**torch.load(data_path/f'learn/seg-chexpert-ft-resnext34-{sz}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Run seg on valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_valid_probs, seg_valid_targs = seg_learn.get_preds(DatasetType.Valid)\n",
    "seg_test_probs, seg_test_targs = seg_learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_thres(input, target, t):\n",
    "    \"dice score with threshold\"\n",
    "    input_hard = input[:,1,:,:] > t\n",
    "    n = input_hard.shape[0]\n",
    "    inputs = input_hard.float().view(n,-1)\n",
    "    targs = target.float().view(n,-1)\n",
    "    intersect = (inputs * targs).sum()\n",
    "    union = (inputs+targs).sum()\n",
    "    dice = ((2. * intersect) / union)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 tensor(0.0289)\n",
      "0.05 tensor(0.4260)\n",
      "0.1 tensor(0.4927)\n",
      "0.15000000000000002 tensor(0.5240)\n",
      "0.2 tensor(0.5377)\n",
      "0.25 tensor(0.5411)\n",
      "0.30000000000000004 tensor(0.5364)\n",
      "0.35000000000000003 tensor(0.5251)\n",
      "0.4 tensor(0.5071)\n",
      "0.45 tensor(0.4817)\n",
      "0.5 tensor(0.4515)\n",
      "0.55 tensor(0.4183)\n",
      "0.6000000000000001 tensor(0.3845)\n",
      "0.65 tensor(0.3515)\n",
      "0.7000000000000001 tensor(0.3188)\n",
      "0.75 tensor(0.2833)\n",
      "0.8 tensor(0.2401)\n",
      "0.8500000000000001 tensor(0.1891)\n",
      "0.9 tensor(0.1287)\n",
      "0.9500000000000001 tensor(0.0641)\n",
      "1.0 tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# dice scores at different probability thresholds\n",
    "for t in np.linspace(0,1,21):\n",
    "    d = dice_thres(seg_valid_probs, seg_valid_targs, t=t)\n",
    "    print(t, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Run clas on valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_valid_probs, clas_valid_targs = clas_learn.get_preds(DatasetType.Valid)\n",
    "clas_test_probs, clas_test_targs = clas_learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Grid search for best thres -  Calculate approx LB score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cand = []\n",
    "for i in np.linspace(0,1,21):\n",
    "    for j in np.linspace(0,1,21):\n",
    "        cand.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thres(cand=cand):\n",
    "    \"grid search cls_thres, seg_thres\"\n",
    "    best_score, best_seg_thres, best_cls_thres = 0, None, None\n",
    "    \n",
    "    for cls_thres, seg_thres in tqdm_notebook(cand):\n",
    "        clas_valid_hard = clas_valid_probs[:, 1] > cls_thres\n",
    "        cm = confusion_matrix(clas_valid_targs, clas_valid_hard)\n",
    "\n",
    "        P_0_0 = cm[0,0] / cm[0, :].sum()\n",
    "        P_1_1 = cm[1,1] / cm[1, :].sum()\n",
    "\n",
    "        # public LB ratios\n",
    "        R_1, R_0 = 0.7886, 0.2114\n",
    "        dice_score = dice_thres(seg_valid_probs, seg_valid_targs, seg_thres)\n",
    "\n",
    "        score_cls = R_1*P_1_1\n",
    "        score_seg = R_0*P_0_0*dice_score\n",
    "        total = score_cls + score_seg\n",
    "\n",
    "        # update\n",
    "        if total > best_score:\n",
    "            best_score = total\n",
    "            best_seg_thres = seg_thres\n",
    "            best_cls_thres = cls_thres\n",
    "\n",
    "    return best_score, best_cls_thres, best_seg_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd97edda719848f5ba2e8baa0063a21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=441), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_score, best_cls_thres, best_seg_thres = find_best_thres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8345), 0.15000000000000002, 0.25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_cls_thres, best_seg_thres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Submit\n",
    "\n",
    "- Convert segmentation probas to binary \n",
    "- Resize to 1024, 1024\n",
    "- Separate instances\n",
    "- Convert individual masks to rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_thres, seg_thres = best_cls_thres, best_seg_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask_functions import *\n",
    "from skimage.measure import label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1377, 224, 224])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all soft mask predictions to hard predictions\n",
    "seg_test_hard = (seg_test_probs[:,1,:,:] > seg_thres).squeeze(1); seg_test_hard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAYAAAAaLWrhAAAABHNCSVQICAgIfAhkiAAAA+5JREFUeJzt3NtR21AUQFGTSRVUQRGZoYE0SxdUkTaSnzw8jI0t28qW7l3riw8MwnO3jl746dvT958HIPGl3gCYmQAhJEAICRBCAoSQACEkQAgJEEIChJAAISRACAkQQgKEkAAh9LXegJm8/Xj/+/Xr80u4JWyFAB/sOLJL3ydCHII+yNuP96vjO34NcxPgnW4J7+PrmZdD0BsJh0cwAW/w6PjEPC8BLnDv4ealn818BHglgbAG54BXmCW+z/5Ot0zWYQJeID7WJMBPWJT/eC/WIcAzZlpwS57e4bEEeEK10PawwNe8EjwjAX5QLq49XegQ4mMI8IgFtZz37D4CZJHX55ddTeqtcx/wt3pPvtVFfW67Xp9f8vdsBAI89PFtyZIdwVZ3GnsiwA2oF3L9+2c2/TlgPf0s/rmZgBHhcThMPgGr6Sc+/pg6QKhNG2B97geHw6QBetyMrZgywP9NdJwz3VXQNabf8VMhnhxhCRNwoVPPQh6H5WMdWGKqAO+dQMcBiYlHmOYQ9J74LsUmRm411QS8hfhY0xQT8Jbpd01Y4uNeJuAJp8LyEQysYfgJuCSazyaaaccahp6Aj4oP1jJ0gNcSH5VhA7x2+omP0rABXkN81IYM8JrpJz62YMgALxEfWzFcgJemn/jYkqECdKOcvRkqwEtMP7ZmmgDFxxZNEaD42KphAjx3/ic+tmyYAE8RH1s3dICwdcMGaPqxB8MEKDj2aJgAYY8ECCEBQmjIAJ0PshfDBSg+9mS4AGFPhvhYQv+GxF4NEaDDTvbKISiEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQugXXtywfSAx46kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "ImageSegment (1, 224, 224)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageSegment(seg_test_hard[4][None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2pil(t):\n",
    "    return (PIL.Image.fromarray((to_np(t)*255).astype(np.uint8), mode='L')\n",
    "                .resize((1024,1024), PIL.Image.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskarray2rles(maskarray):\n",
    "    rles = []\n",
    "    labeled_maskarray = label(maskarray)\n",
    "    unique_labels = list(np.unique(labeled_maskarray)[1:])\n",
    "    for l in unique_labels:\n",
    "        rle = mask2rle((labeled_maskarray == l).astype(np.uint8)*255, *(1024,1024))\n",
    "        rles.append(rle)\n",
    "    return rles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract test image ids\n",
    "test_image_ids = np.array([o.stem for o in seg_learn.data.test_ds.items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find images to submit mask predictions for\n",
    "has_pneumo = to_np((clas_test_probs[:, 1] < cls_thres)).astype(bool)\n",
    "\n",
    "mask_test_image_ids = test_image_ids[has_pneumo]\n",
    "mask_test_hard = seg_test_hard[tensor(has_pneumo)]\n",
    "\n",
    "no_mask_test_image_ids = test_image_ids[~has_pneumo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150,), (1227,), torch.Size([150, 224, 224]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_test_image_ids.shape, no_mask_test_image_ids.shape, mask_test_hard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12967b39638340f088d20b32ce8dd911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create rle for all test predictions\n",
    "all_image_ids = []\n",
    "all_rles = []\n",
    "for i in tqdm_notebook(range(len(mask_test_image_ids))):    \n",
    "    image_id, t = mask_test_image_ids[i], mask_test_hard[i]\n",
    "    maskimg = tensor2pil(t)\n",
    "    maskarray = np.asarray(maskimg).T\n",
    "    if len(np.unique(maskarray)) != 1: rles = maskarray2rles(maskarray)\n",
    "    else: rles = ['-1']\n",
    "    for rle in rles:\n",
    "        all_image_ids.append(image_id)\n",
    "        all_rles.append(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339, 339)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_image_ids), len(all_rles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.DataFrame({\"ImageId\": all_image_ids, \"EncodedPixels\":all_rles})\n",
    "sub2 = pd.DataFrame({\"ImageId\": no_mask_test_image_ids, \"EncodedPixels\":\"-1\"})\n",
    "\n",
    "final_sub = pd.concat([sub1, sub2])\n",
    "final_sub.to_csv(\"sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub2 = pd.DataFrame({\"ImageId\": no_mask_test_image_ids, \"EncodedPixels\":\"-1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub = pd.concat([sub1, sub2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub.to_csv(\"sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sub.csv' target='_blank'>sub.csv</a><br>"
      ],
      "text/plain": [
       "/home/turgutluk/git/siim_acr_pneu/sub.csv"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(\"sub.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_fastai]",
   "language": "python",
   "name": "conda-env-my_fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
